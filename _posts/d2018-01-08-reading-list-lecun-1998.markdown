---
title: "Reading List: 1998 Yann LeCun"
layout: post
tag: papers
blog: true
author: Olga Krieger
summary: "Reading List: recaps of important data science papers."
permalink: reading-list
---

<h2> Reading List: 1998 Yann LeCun </h2>

This is the first post where I recap important papers as part of continuing my data science education.

I start with the 1998 Yann LeCun study as it advances a deep learning class that is essentially computersâ€™ vision and hearing, enabling real-life applications such as safe self-driving cars and reading of radiology images.

<p>In discussing solutions to high-dimensional pattern recognition, ie handwritten character or speech, it shows that automatic learning machines that operate directly on pixel images are more accurate than hand-crafted individually designed feature extraction modules.
</p>

<p>It thus suggested a then-new paradigm of globally trained Graph Transformer Networks, the core of which is a Convolutional Neural Network. 
</p>

<p>The three conditions that allowed for this progress are: one, low-cost machines with brute-force arithmetic methods; two, large databases for problems with a wide market interest and three, powerful machine learning techniques that can handle high-dimensional inputs and can generate intricate decision functions.
</p>

Nuggets:

*  While more automatic learning is beneficial, no learning technique can succeed without a minimal prior knowledge about the task. 

*  **Gradient-based learning** draws on the fact that it is much easier to minimize the loss on a smooth continuous function than a discrete one, as estimated by the impact of small variations of the parameter values.

*  The basic idea of *back-propagation* is that gradients can be computed efficiently by propagation from the output to input.
</p>

<p>Keywords: Neural Networks, OCR, Document Recognition, Machine Learning, Gradient-Based Learning, Convolutional Neural Networks, Graph Transformer Networks, Finite State Transducers.
</p>
