---
title: "Reading List: 1998 Yann LeCun"
layout: post
tag: papers
blog: true
author: Olga Krieger
summary: "Reading List: recaps of important data science papers."
permalink: reading-list
jemoji: '<img class="emoji" title=":books:" alt=":books:" src="https://assets.github.com/images/icons/emoji/unicode/1f4da.png" height="20" width="20" align="absmiddle">'
---

<p>As most graduate students, we read many papers over the course of our PhD and there are few papers which seem to be worthwhile and important. Thishere will be a compilation of what I ahve enjoyed reading in various fields of physics. 
</p>

<h2> Reading List: 1998 Yann LeCun </h2>

<p>This is the first post where I recap important papers as part of continuing my data science education.
</p>

<p>I start with the 1998 Yann LeCun study as it advances a deep learning class that is essentially computersâ€™ vision and hearing, enabling real-life applications such as safe self-driving cars and reading of radiology images.
</p>

<p>In discussing solutions to high-dimensional pattern recognition, ie handwritten character or speech, it shows that automatic learning machines that operate directly on pixel images are more accurate than hand-crafted individually designed feature extraction modules.
</p>

<p>It thus suggested a then-new paradigm of globally trained Graph Transformer Networks, the core of which is a Convolutional Neural Network. 
</p>

<p>The three conditions that allowed for this progress are: one, low-cost machines with brute-force arithmetic methods; two, large databases for problems with a wide market interest and three, powerful machine learning techniques that can handle high-dimensional inputs and can generate intricate decision functions.
</p>

<p>Nuggets:
- While more automatic learning is beneficial, no learning technique can succeed without a minimal prior knowledge about the task. 
- **Gradient-based learning** draws on the fact that it is much easier to minimize the loss on a smooth continuous function than a discrete one, as estimated by the impact of small variations of the parameter values.
- The basic idea of *back-propagation* is that gradients can be computed efficiently by propagation from the output to input.
</p>

<p>Keywords: Neural Networks, OCR, Document Recognition, Machine Learning, Gradient-Based Learning, Convolutional Neural Networks, Graph Transformer Networks, Finite State Transducers.
</p>
